{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07d02eba-75d7-4cf5-aa19-0ffc23384ea5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List, Tuple, Dict\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm import tqdm\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c27350-3c04-486a-a19d-a4b3edcf634d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=\"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\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c206d22b-f7c4-4bee-b523-9cd5dfedc77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 1. Paragraph Chunking\n",
    "# =========================\n",
    "\n",
    "def paragraph_chunking(text: str, max_tokens: int = 512, tokenizer=None) -> List[str]:\n",
    "    \"\"\"\n",
    "    Splits text into chunks based on paragraphs, ensuring each chunk is within max_tokens.\n",
    "    If a paragraph is too long, it is further split by sentences.\n",
    "    \"\"\"\n",
    "    assert tokenizer is not None, \"Tokenizer must be provided\"\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    chunks = []\n",
    "    for paragraph in paragraphs:\n",
    "        tokens = tokenizer.encode(paragraph, add_special_tokens=False)\n",
    "        if len(tokens) <= max_tokens:\n",
    "            chunks.append(paragraph)\n",
    "        else:\n",
    "            # If paragraph is too long, split by sentences (simple split, can use nltk for better splitting)\n",
    "            sentences = [s.strip() for s in paragraph.split('.') if s.strip()]\n",
    "            current_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                if not sentence.endswith('.'):\n",
    "                    sentence += '.'\n",
    "                test_chunk = (current_chunk + \" \" + sentence).strip() if current_chunk else sentence\n",
    "                if len(tokenizer.encode(test_chunk, add_special_tokens=False)) > max_tokens:\n",
    "                    if current_chunk:\n",
    "                        chunks.append(current_chunk.strip())\n",
    "                    current_chunk = sentence\n",
    "                else:\n",
    "                    current_chunk = test_chunk\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk.strip())\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b9a41d7-be15-467b-87db-6154ffc8fca3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 2. Embedding Utilities\n",
    "# =========================\n",
    "\n",
    "def mean_pooling(model_output, attention_mask):\n",
    "    \"\"\"\n",
    "    Applies mean pooling to the output of a transformer model.\n",
    "    \"\"\"\n",
    "    token_embeddings = model_output.last_hidden_state\n",
    "    input_mask_expanded = attention_mask.unsqueeze(-1).expand(token_embeddings.size()).float()\n",
    "    sum_embeddings = torch.sum(token_embeddings * input_mask_expanded, 1)\n",
    "    sum_mask = torch.clamp(input_mask_expanded.sum(1), min=1e-9)\n",
    "    return sum_embeddings / sum_mask\n",
    "\n",
    "def get_embeddings(\n",
    "    texts: List[str],\n",
    "    tokenizer,\n",
    "    model,\n",
    "    batch_size: int = 16,\n",
    "    device: str = \"cpu\"\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Generates embeddings for a list of texts using Legal-BERT with mean pooling.\n",
    "    \"\"\"\n",
    "    all_embeddings = []\n",
    "    model = model.to(device)\n",
    "    for i in tqdm(range(0, len(texts), batch_size), desc=\"Embedding chunks\"):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        encoded = tokenizer(batch, padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "        encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "        with torch.no_grad():\n",
    "            output = model(**encoded)\n",
    "            embeddings = mean_pooling(output, encoded['attention_mask'])\n",
    "        all_embeddings.append(embeddings.cpu().numpy())\n",
    "    return np.vstack(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d215caad-ae10-4d39-9c93-a68c69c6a4a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# 3. Data Loading & Processing\n",
    "# =========================\n",
    "\n",
    "def load_and_chunk_documents(\n",
    "    directory: str,\n",
    "    tokenizer,\n",
    "    max_tokens: int = 512\n",
    ") -> Tuple[List[str], List[Tuple[str, int]]]:\n",
    "    \"\"\"\n",
    "    Reads all text files in a directory, splits them into paragraph-based chunks,\n",
    "    and returns the chunks and their source mapping.\n",
    "    Returns:\n",
    "        chunks: List of text chunks.\n",
    "        mapping: List of (filename, chunk_idx) tuples for each chunk.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    mapping = []\n",
    "    for filename in tqdm(os.listdir(directory), desc=\"Reading files\"):\n",
    "        if filename.endswith('.txt'):\n",
    "            with open(os.path.join(directory, filename), \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "            file_chunks = paragraph_chunking(text, max_tokens=max_tokens, tokenizer=tokenizer)\n",
    "            chunks.extend(file_chunks)\n",
    "            mapping.extend([(filename, idx) for idx in range(len(file_chunks))])\n",
    "    return chunks, mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cccf7b8-93db-4f8c-9e2e-964678fff657",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings_and_chunks(\n",
    "    data_dir: str,\n",
    "    embedding_model: str = \"nlpaueb/legal-bert-base-uncased\",\n",
    "    chunk_tokens: int = 512,\n",
    "    device: str = \"cpu\",\n",
    "    save_path: str = \"embeddings_and_chunks.pkl\"\n",
    "):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "    model = AutoModel.from_pretrained(embedding_model)\n",
    "    chunks, mapping = load_and_chunk_documents(data_dir, tokenizer, chunk_tokens)\n",
    "    chunk_embeddings = get_embeddings(chunks, tokenizer, model, device=device)\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump({\"embeddings\": chunk_embeddings, \"chunks\": chunks, \"mapping\": mapping}, f)\n",
    "    print(f\"Saved embeddings and chunks to {save_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2cc138b-9b57-4569-900f-725ca2b32c78",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_embeddings_and_chunks(\n",
    "    data_dir=\"data/CUAD_v1/full_contract_txt\",\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a81683a-2f5e-4939-af01-d8475db484fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_similar_chunks(\n",
    "    query: str,\n",
    "    chunk_embeddings: np.ndarray,\n",
    "    chunks: List[str],\n",
    "    tokenizer,\n",
    "    model,\n",
    "    top_k: int = 5,\n",
    "    device: str = \"cpu\"\n",
    ") -> List[Tuple[str, float, int]]:\n",
    "    model = model.to(device)\n",
    "    encoded = tokenizer([query], padding=True, truncation=True, max_length=512, return_tensors=\"pt\")\n",
    "    encoded = {k: v.to(device) for k, v in encoded.items()}\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "        query_embedding = mean_pooling(output, encoded['attention_mask']).cpu().numpy()\n",
    "    similarities = cosine_similarity(query_embedding, chunk_embeddings)[0]\n",
    "    top_indices = similarities.argsort()[::-1][:top_k]\n",
    "    return [(chunks[i], similarities[i], i) for i in top_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b2dfbd0-3b39-4bc1-8a57-845816b924b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_query(\n",
    "    query: str,\n",
    "    embedding_model: str = \"nlpaueb/legal-bert-base-uncased\",\n",
    "    embedding_path: str = \"embeddings_and_chunks.pkl\",\n",
    "    llm_model: str = \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    hf_token: str = None,\n",
    "    top_k: int = 5,\n",
    "    device: str = \"cpu\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Loads embeddings, retrieves relevant chunks, and generates an answer using a gated LLM.\n",
    "    Hugging Face token is passed ONLY to from_pretrained() for gated models.\n",
    "    \"\"\"\n",
    "\n",
    "    # Load embeddings and chunks\n",
    "    with open(embedding_path, \"rb\") as f:\n",
    "        data = pickle.load(f)\n",
    "    chunk_embeddings = data[\"embeddings\"]\n",
    "    chunks = data[\"chunks\"]\n",
    "\n",
    "    # Load embedding model and tokenizer (no token needed for Legal-BERT)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(embedding_model)\n",
    "    model = AutoModel.from_pretrained(embedding_model)\n",
    "\n",
    "    # Search\n",
    "    top_chunks = search_similar_chunks(query, chunk_embeddings, chunks, tokenizer, model, top_k, device)\n",
    "    relevant_texts = [chunk for chunk, _, _ in top_chunks]\n",
    "\n",
    "    # Load LLM and tokenizer WITH token (for gated repo)\n",
    "    llm_tokenizer = AutoTokenizer.from_pretrained(llm_model, token=hf_token)\n",
    "    llm_model_obj = AutoModelForCausalLM.from_pretrained(llm_model, token=hf_token)\n",
    "\n",
    "    # Create pipeline WITHOUT token\n",
    "    llm = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=llm_model_obj,\n",
    "        tokenizer=llm_tokenizer,\n",
    "        device=device\n",
    "    )\n",
    "\n",
    "    # Mistral Instruct expects [INST] ... [/INST] prompt format\n",
    "    context = \"\\n\\n\".join(relevant_texts)\n",
    "    prompt = f\"[INST] Context:\\n{context}\\n\\nQuestion: {query}\\n\\nAnswer: [/INST]\"\n",
    "    response = llm(prompt, max_new_tokens=256, do_sample=True, temperature=0.2)\n",
    "    answer = response[0]['generated_text'].split(\"Answer:\")[-1].strip()\n",
    "\n",
    "    print(\"Top Chunks Used:\\n\")\n",
    "    for i, (chunk, score, idx) in enumerate(top_chunks, 1):\n",
    "        print(f\"Rank {i} (Score: {score:.4f}):\\n{chunk}\\n{'-'*40}\")\n",
    "    print(\"\\nLLM Answer:\\n\", answer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "741de2e7-c3fa-46f1-a24d-cfe16341904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_query(\n",
    "    query=\"In MARKETING AFFILIATE AGREEMENT which parties are involved?\",\n",
    "    embedding_model=\"nlpaueb/legal-bert-base-uncased\",\n",
    "    embedding_path=\"embeddings_and_chunks.pkl\",\n",
    "    llm_model=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    hf_token=\"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\",\n",
    "    top_k=5,\n",
    "    device=\"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "491bd273-2f62-41c2-9b20-83f9845c4e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
